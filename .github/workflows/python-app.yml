# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Deploy to S3

on:
  push:
    branches:
      - main  # adjust this to your main branch name

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Deploy to MinIO Bucket
        env:
          MC_DEBUG: 1
        run: |
          # Set up MinIO client (mc)
          wget https://dl.min.io/client/mc/release/linux-amd64/mc
          chmod +x mc
          
          # Configure MinIO client with access credentials
          ./mc alias set osclimate https://minio-api-datamesh-demo1.apps.osc-cl4.apps.os-climate.org ${{ secrets.AWS_ACCESS_KEY_ID }} ${{ secrets.AWS_SECRET_ACCESS_KEY }} --insecure --api S3v4
          
          # ./mc --insecure mb osclimate/airflow/release/dags 

          # ./mc --insecure --recursive ls osclimate/airflow

          # Copy files to MinIO bucket
          ./mc cp -r ./dags/ osclimate/airflow/release/dags --insecure 
      # - name: Configure AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v1
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: https://minio-os-climate.apps.osc-cl4.apps.os-climate.org

      # - name: Deploy to S3
      #   uses: aws-actions/amazon-s3-deploy@v0.1.1
      #   with:
      #     args: --acl public-read --follow-symlinks --delete
      #     bucket-name: airflow/release/dags
      #     local-dir: dags
      #     include: '*.py, *.ipynb'
      #     exclude: '*.tmp, config.yml'
